{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd8adcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import MinkowskiEngine as ME\n",
    "import MinkowskiEngine.MinkowskiFunctional as MF\n",
    "\n",
    "\n",
    "import open3d as o3d\n",
    "import os\n",
    "import torch\n",
    "import einops\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c909f062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_indices(base_path, train_folder, gt_folder, train_prefix=None, gt_prefix=None, format=\".txt\"):\n",
    "    \n",
    "    '''\n",
    "        This function assumes the following file structure:\n",
    "        \n",
    "        base_path\n",
    "        -train_folder\n",
    "            -train_prefix + {i} + .txt\n",
    "        -gt_folder\n",
    "            -gt_prefix + {i} + .txt\n",
    "            \n",
    "        And the following file contents:\n",
    "        \n",
    "        GT:\n",
    "            a single integer corresponding to a category\n",
    "        Train:\n",
    "            N lines containing 3 comma-separated floats corresponding to point coordinates\n",
    "    '''\n",
    "    import os\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    #\n",
    "    train_prefix = train_prefix or train_folder \n",
    "    gt_prefix = gt_prefix or gt_folder \n",
    "    \n",
    "    #\n",
    "    train_path = os.path.join(base_path, train_folder)\n",
    "    gt_path = os.path.join(base_path, gt_folder)\n",
    "    \n",
    "    #\n",
    "    gt_file = lambda i : os.path.join(gt_path, gt_prefix + str(i) + format)\n",
    "    train_file = lambda i : os.path.join(train_path, train_prefix + str(i) + format)\n",
    "    \n",
    "    #\n",
    "    categories = {}\n",
    "    \n",
    "    #\n",
    "    samples = os.listdir(train_path)\n",
    "    N = len(samples)\n",
    "    \n",
    "    for i in tqdm(range(1, N+1)):\n",
    "        with open(gt_file(i)) as GT:\n",
    "            \n",
    "            cat = GT.readline()[0]\n",
    "            if cat not in categories.keys(): \n",
    "                categories[cat] = [i]\n",
    "            else:\n",
    "                categories[cat].append(i)\n",
    "    \n",
    "    save_path = os.path.join(base_path,\"indices.txt\");\n",
    "    \n",
    "    with open(save_path, \"w\") as F:\n",
    "        for key in categories.keys():\n",
    "            F.write(\",\".join(map(str, categories[key])) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "282e7843",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46000/46000 [00:00<00:00, 87914.71it/s]\n"
     ]
    }
   ],
   "source": [
    "# Creating the dataset splits\n",
    "base = \"/home/ioannis/Desktop/programming/data/SHREC/SHREC2022/dataset/training/\"\n",
    "category_indices(base, \"pointCloud\", \"GTpointCloud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a4a5525",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SHREC2022Dataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, path, train=True, valid=False, valid_split=0.2):\n",
    "        \n",
    "        self.path = os.path.join(path, \"training\" if train else \"test\")\n",
    "        self.pc_prefix = \"pointCloud\"\n",
    "        self.gt_prefix = \"GTpointCloud\"\n",
    "        self.format = \".txt\"\n",
    "        self.valid = valid if train else False\n",
    "        self.size = 0 if train else len(os.listdir(self.path))\n",
    "        \n",
    "        #check if an existing train-validation split matches the one given\n",
    "        split_info_file = os.path.join(path, \"training/split_info.txt\")\n",
    "        self.t_savefile = os.path.join(path, \"training/train_split.txt\")\n",
    "        self.v_savefile = os.path.join(path, \"training/valid_split.txt\")\n",
    "        if os.path.exists(split_info_file):\n",
    "            with open(split_info_file) as F:\n",
    "                v, vsize, tsize = list(map(float, F.readline().split(',')))\n",
    "\n",
    "                if v == valid_split:\n",
    "                    print(\"Specified split already exists. Using the existing one.\")\n",
    "                    self.size = int(vsize if self.valid else tsize)\n",
    "                    return\n",
    "        \n",
    "        if train:\n",
    "            print(\"Creating a new train-validation split.\")\n",
    "            import random\n",
    "            with open(os.path.join(path, \"training/indices.txt\")) as F, open(self.t_savefile, \"w\") as T,\\\n",
    "                open(self.v_savefile, \"w\") as V, open(split_info_file, \"w\") as I:\n",
    "\n",
    "                lines = F.readlines()\n",
    "                cat_sz = len(lines[0].split(\",\"))\n",
    "                train_sz = int(cat_sz * (1-valid_split))\n",
    "\n",
    "                v_indices = []\n",
    "                t_indices = []\n",
    "\n",
    "                for line in lines:\n",
    "                    line = list(map(int, line.split(\",\")))\n",
    "                    random.shuffle(line)\n",
    "                    v_indices = v_indices + line[train_sz:]\n",
    "                    t_indices = t_indices + line[:train_sz]\n",
    "\n",
    "                self.size = len(v_indices) if valid else len(t_indices)\n",
    "                T.write(\"\\n\".join(map(str, t_indices)))\n",
    "                V.write(\"\\n\".join(map(str, v_indices)))\n",
    "                \n",
    "                I.write(str(valid_split)+','+str(len(v_indices))+','+str(len(t_indices)))\n",
    "\n",
    "                \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        with open(self.v_savefile if self.valid else self.t_savefile, \"r\") as F:\n",
    "            index = F.readlines()[index]\n",
    "            index = int(index) if '\\n' not in index else int(index[:-1])\n",
    "         \n",
    "        \n",
    "        #assembling the file name for the data and labels\n",
    "        pc_name = os.path.join(self.path, self.pc_prefix, self.pc_prefix + str(index) + self.format)\n",
    "        gt_name = os.path.join(self.path, self.gt_prefix, self.gt_prefix + str(index) + self.format)\n",
    "        \n",
    "        #parsing the point cloud\n",
    "        pcloud = self.parse_point_cloud(pc_name)\n",
    "        label = self.parse_label(gt_name)\n",
    "\n",
    "        data = {\"x\": pcloud, \"y\": label['data']}\n",
    "        \n",
    "        for t in self.transform:\n",
    "            data = t(data)\n",
    "        \n",
    "        return data\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.size\n",
    "    \n",
    "    def transform(self, data):\n",
    "        \n",
    "        return self.unit_sphere_normalize(data)\n",
    "        \n",
    "    def unit_sphere_normalize(self, x):\n",
    "        \n",
    "        max_norm = (x[\"x\"]*x[\"x\"]).sum(-1).max().sqrt()\n",
    "        x[\"x\"] /= max_norm\n",
    "        \n",
    "        x[\"norm_factor\"] = max_norm\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def minkowski_collate(list_data):\n",
    "    coordinates, features, labels = ME.utils.sparse_collate(\n",
    "        [d['x'] for d in list_data],\n",
    "        [d['x'] for d in list_data],\n",
    "        [d['y'][0].unsqueeze(0) for d in list_data],\n",
    "        dtype = torch.float32\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"coordinates\": coordinates, \n",
    "        \"features\"   : features,\n",
    "        \"labels\"     : labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f89a7973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specified split already exists. Using the existing one.\n",
      "Specified split already exists. Using the existing one.\n",
      "36800\n",
      "9200\n"
     ]
    }
   ],
   "source": [
    "path = \"/home/ioannis/Desktop/programming/data/SHREC/SHREC2022/dataset\"\n",
    "t_dataset = SHREC2022Dataset(path, train=True, valid=False, valid_split=0.2)\n",
    "v_dataset = SHREC2022Dataset(path, train=True, valid=True, valid_split=0.2)\n",
    "\n",
    "sample1 = t_dataset[0]\n",
    "sample2 = v_dataset[0]\n",
    "# show_point_cloud_o3d(sample1['x'])\n",
    "# show_point_cloud_o3d(sample2['x'])\n",
    "print(len(t_dataset))\n",
    "print(len(v_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8f6fd7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_loader = DataLoader(t_dataset, batch_size=batch_size, shuffle=True, collate_fn = minkowski_collate, num_workers=8)\n",
    "eval_loader = DataLoader(v_dataset, batch_size=batch_size, shuffle=False, collate_fn=minkowski_collate, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aeb42030",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2300/2300 [01:27<00:00, 26.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87.89322829246521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# time to parse the whole dataset\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "t1 = time.time()\n",
    "for batch in tqdm(train_loader):\n",
    "    pass\n",
    "print(time.time() - t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cb2290",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd5407d",
   "metadata": {},
   "source": [
    "## PointNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "837d2446",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinkowskiPointNet(ME.MinkowskiNetwork):\n",
    "    def __init__(self, in_channel, out_channel, embedding_channel=1024, dimension=3):\n",
    "        ME.MinkowskiNetwork.__init__(self, dimension)\n",
    "        self.conv1 = nn.Sequential(\n",
    "            ME.MinkowskiLinear(3, 64, bias=False),\n",
    "            ME.MinkowskiBatchNorm(64),\n",
    "            ME.MinkowskiReLU(),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            ME.MinkowskiLinear(64, 64, bias=False),\n",
    "            ME.MinkowskiBatchNorm(64),\n",
    "            ME.MinkowskiReLU(),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            ME.MinkowskiLinear(64, 64, bias=False),\n",
    "            ME.MinkowskiBatchNorm(64),\n",
    "            ME.MinkowskiReLU(),\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            ME.MinkowskiLinear(64, 128, bias=False),\n",
    "            ME.MinkowskiBatchNorm(128),\n",
    "            ME.MinkowskiReLU(),\n",
    "        )\n",
    "        self.conv5 = nn.Sequential(\n",
    "            ME.MinkowskiLinear(128, embedding_channel, bias=False),\n",
    "            ME.MinkowskiBatchNorm(embedding_channel),\n",
    "            ME.MinkowskiReLU(),\n",
    "        )\n",
    "        self.max_pool = ME.MinkowskiGlobalMaxPooling()\n",
    "\n",
    "        self.linear1 = nn.Sequential(\n",
    "            ME.MinkowskiLinear(embedding_channel, 512, bias=False),\n",
    "            ME.MinkowskiBatchNorm(512),\n",
    "            ME.MinkowskiReLU(),\n",
    "        )\n",
    "        self.dp1 = ME.MinkowskiDropout()\n",
    "        self.linear2 = ME.MinkowskiLinear(512, out_channel, bias=True)\n",
    "\n",
    "    def forward(self, x: ME.TensorField):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.max_pool(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.dp1(x)\n",
    "        return self.linear2(x).F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f070da7",
   "metadata": {},
   "source": [
    "## Minkowski Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40fa0d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinkowskiFCNN(ME.MinkowskiNetwork):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channel,\n",
    "        out_channel,\n",
    "        embedding_channel=1024,\n",
    "        channels=(32, 48, 64, 96, 128),\n",
    "        D=3,\n",
    "    ):\n",
    "        ME.MinkowskiNetwork.__init__(self, D)\n",
    "\n",
    "        self.network_initialization(\n",
    "            in_channel,\n",
    "            out_channel,\n",
    "            channels=channels,\n",
    "            embedding_channel=embedding_channel,\n",
    "            kernel_size=3,\n",
    "            D=D,\n",
    "        )\n",
    "        self.weight_initialization()\n",
    "\n",
    "    def get_mlp_block(self, in_channel, out_channel):\n",
    "        return nn.Sequential(\n",
    "            ME.MinkowskiLinear(in_channel, out_channel, bias=False),\n",
    "            ME.MinkowskiBatchNorm(out_channel),\n",
    "            ME.MinkowskiLeakyReLU(),\n",
    "        )\n",
    "\n",
    "    def get_conv_block(self, in_channel, out_channel, kernel_size, stride):\n",
    "        return nn.Sequential(\n",
    "            ME.MinkowskiConvolution(\n",
    "                in_channel,\n",
    "                out_channel,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride,\n",
    "                dimension=self.D,\n",
    "            ),\n",
    "            ME.MinkowskiBatchNorm(out_channel),\n",
    "            ME.MinkowskiLeakyReLU(),\n",
    "        )\n",
    "\n",
    "    def network_initialization(\n",
    "        self,\n",
    "        in_channel,\n",
    "        out_channel,\n",
    "        channels,\n",
    "        embedding_channel,\n",
    "        kernel_size,\n",
    "        D=3,\n",
    "    ):\n",
    "        self.mlp1 = self.get_mlp_block(in_channel, channels[0])\n",
    "        self.conv1 = self.get_conv_block(\n",
    "            channels[0],\n",
    "            channels[1],\n",
    "            kernel_size=kernel_size,\n",
    "            stride=1,\n",
    "        )\n",
    "        self.conv2 = self.get_conv_block(\n",
    "            channels[1],\n",
    "            channels[2],\n",
    "            kernel_size=kernel_size,\n",
    "            stride=2,\n",
    "        )\n",
    "\n",
    "        self.conv3 = self.get_conv_block(\n",
    "            channels[2],\n",
    "            channels[3],\n",
    "            kernel_size=kernel_size,\n",
    "            stride=2,\n",
    "        )\n",
    "\n",
    "        self.conv4 = self.get_conv_block(\n",
    "            channels[3],\n",
    "            channels[4],\n",
    "            kernel_size=kernel_size,\n",
    "            stride=2,\n",
    "        )\n",
    "        self.conv5 = nn.Sequential(\n",
    "            self.get_conv_block(\n",
    "                channels[1] + channels[2] + channels[3] + channels[4],\n",
    "                embedding_channel // 4,\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "            ),\n",
    "            self.get_conv_block(\n",
    "                embedding_channel // 4,\n",
    "                embedding_channel // 2,\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "            ),\n",
    "            self.get_conv_block(\n",
    "                embedding_channel // 2,\n",
    "                embedding_channel,\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.pool = ME.MinkowskiMaxPooling(kernel_size=3, stride=2, dimension=D)\n",
    "\n",
    "        self.global_max_pool = ME.MinkowskiGlobalMaxPooling()\n",
    "        self.global_avg_pool = ME.MinkowskiGlobalAvgPooling()\n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            self.get_mlp_block(embedding_channel * 2, 512),\n",
    "            ME.MinkowskiDropout(),\n",
    "            self.get_mlp_block(512, 512),\n",
    "            ME.MinkowskiLinear(512, out_channel, bias=True),\n",
    "        )\n",
    "\n",
    "        # No, Dropout, last 256 linear, AVG_POOLING 92%\n",
    "\n",
    "    def weight_initialization(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, ME.MinkowskiConvolution):\n",
    "                ME.utils.kaiming_normal_(m.kernel, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "\n",
    "            if isinstance(m, ME.MinkowskiBatchNorm):\n",
    "                nn.init.constant_(m.bn.weight, 1)\n",
    "                nn.init.constant_(m.bn.bias, 0)\n",
    "\n",
    "    def forward(self, x: ME.TensorField):\n",
    "        x = self.mlp1(x)\n",
    "        y = x.sparse()\n",
    "\n",
    "        y = self.conv1(y)\n",
    "        y1 = self.pool(y)\n",
    "\n",
    "        y = self.conv2(y1)\n",
    "        y2 = self.pool(y)\n",
    "\n",
    "        y = self.conv3(y2)\n",
    "        y3 = self.pool(y)\n",
    "\n",
    "        y = self.conv4(y3)\n",
    "        y4 = self.pool(y)\n",
    "\n",
    "        x1 = y1.slice(x)\n",
    "        x2 = y2.slice(x)\n",
    "        x3 = y3.slice(x)\n",
    "        x4 = y4.slice(x)\n",
    "\n",
    "        x = ME.cat(x1, x2, x3, x4)\n",
    "\n",
    "        y = self.conv5(x.sparse())\n",
    "        x1 = self.global_max_pool(y)\n",
    "        x2 = self.global_avg_pool(y)\n",
    "\n",
    "        return self.final(ME.cat(x1, x2)).F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "841bf972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "minkpointnet = MinkowskiFCNN(in_channel = 3, out_channel = 5).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acadb477",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86e64a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_batch(batch, device=\"cuda\", quantization_size=0.05):\n",
    "    batch[\"coordinates\"][:, 1:] = batch[\"coordinates\"][:, 1:] / quantization_size\n",
    "    return ME.TensorField(\n",
    "        coordinates=batch[\"coordinates\"],\n",
    "        features=batch[\"features\"],\n",
    "        device=device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efbf27ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2300/2300 [02:23<00:00, 16.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 - loss: 0.4326362850063521\n",
      "train accuracy 0.8411141633987427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss: 0.534489544911877\n",
      "validation_acc : 0.800000011920929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2300/2300 [02:26<00:00, 15.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 - loss: 0.3273414047167677\n",
      "train accuracy 0.8791847825050354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss: 0.6103396369776\n",
      "validation_acc : 0.783152163028717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2300/2300 [02:22<00:00, 16.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 - loss: 0.24028613207983257\n",
      "train accuracy 0.9142934679985046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss: 0.6943070894609327\n",
      "validation_acc : 0.7771739363670349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2300/2300 [02:21<00:00, 16.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 - loss: 0.1814176823557152\n",
      "train accuracy 0.9369293451309204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss: 0.7306370152549251\n",
      "validation_acc : 0.7794565558433533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2300/2300 [02:22<00:00, 16.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 - loss: 0.1409721631496011\n",
      "train accuracy 0.950951099395752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss: 0.7489643728368632\n",
      "validation_acc : 0.7871739268302917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2300/2300 [02:22<00:00, 16.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 - loss: 0.11780726776245738\n",
      "train accuracy 0.9588587284088135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss: 0.8387530057608028\n",
      "validation_acc : 0.7830435037612915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2300/2300 [02:30<00:00, 15.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 - loss: 0.09607708493594344\n",
      "train accuracy 0.9666032791137695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss: 0.8981141074746847\n",
      "validation_acc : 0.7873913049697876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2300/2300 [02:22<00:00, 16.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 - loss: 0.08784098507841523\n",
      "train accuracy 0.968668520450592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss: 0.8608694858460323\n",
      "validation_acc : 0.7872826457023621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2300/2300 [02:21<00:00, 16.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 - loss: 0.0763776102642473\n",
      "train accuracy 0.9742391705513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss: 0.9793378312728853\n",
      "validation_acc : 0.7733696103096008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2300/2300 [02:20<00:00, 16.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 - loss: 0.07015811979113926\n",
      "train accuracy 0.9764402508735657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss: 0.9559305762685836\n",
      "validation_acc : 0.786195695400238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2300/2300 [02:21<00:00, 16.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 - loss: 0.060614798235107195\n",
      "train accuracy 0.9796739220619202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss: 1.002106832701551\n",
      "validation_acc : 0.7865217328071594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2300/2300 [02:21<00:00, 16.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11 - loss: 0.0595244886492646\n",
      "train accuracy 0.9802989363670349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss: 0.9706727385196997\n",
      "validation_acc : 0.7850000262260437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2300/2300 [02:21<00:00, 16.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12 - loss: 0.05313013940632632\n",
      "train accuracy 0.9819021821022034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss: 1.0979773423083774\n",
      "validation_acc : 0.7754347920417786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2300/2300 [02:21<00:00, 16.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13 - loss: 0.0461304036746309\n",
      "train accuracy 0.984646737575531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss: 1.0906997536892153\n",
      "validation_acc : 0.7866304516792297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2300/2300 [02:21<00:00, 16.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14 - loss: 0.044197986306453066\n",
      "train accuracy 0.9851087331771851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss: 1.1075005747884026\n",
      "validation_acc : 0.7836956977844238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2300/2300 [02:21<00:00, 16.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15 - loss: 0.04246408314610048\n",
      "train accuracy 0.9853532910346985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss: 1.1423766938583297\n",
      "validation_acc : 0.7781521677970886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2300/2300 [02:21<00:00, 16.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16 - loss: 0.042558459441396065\n",
      "train accuracy 0.9859783053398132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss: 1.1205272277582274\n",
      "validation_acc : 0.7783696055412292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2300/2300 [02:22<00:00, 16.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17 - loss: 0.040303393152497174\n",
      "train accuracy 0.9866847991943359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss: 1.1245689939019148\n",
      "validation_acc : 0.7731521725654602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2300/2300 [02:26<00:00, 15.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18 - loss: 0.03355003540808154\n",
      "train accuracy 0.9887500405311584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss: 1.1468779277158003\n",
      "validation_acc : 0.782608687877655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2300/2300 [02:25<00:00, 15.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19 - loss: 0.0353776682241859\n",
      "train accuracy 0.9886141419410706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_loss: 1.1089505644129467\n",
      "validation_acc : 0.7823913097381592\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "cls_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "param_loss = torch.nn.MSELoss()\n",
    "\n",
    "num_epochs = 20\n",
    "optimizer = torch.optim.Adam(minkpointnet.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    \n",
    "    m_loss = 0\n",
    "    acc = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        #print(\"Loading data time: \", time.time() - t_e)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        labels = batch[\"labels\"].long().to(device)\n",
    "        \n",
    "        batch_size = labels.shape[0]\n",
    "        \n",
    "        minknet_input = create_input_batch(\n",
    "            batch, \n",
    "            device=device,\n",
    "            quantization_size=0.05\n",
    "        )\n",
    "        #minknet_input = ME.TensorField(\n",
    "        #    coordinates=batch[\"coordinates\"].to(device),\n",
    "        #    features   =batch[\"features\"].to(device)\n",
    "        #)\n",
    "\n",
    "        pred = minkpointnet(minknet_input)\n",
    "        #print(\"Activation time: \",time.time() - t)\n",
    "        \n",
    "        loss = cls_loss(pred, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        m_loss += loss.item() #/ batch_size\n",
    "        \n",
    "        acc += (torch.max(pred, dim=-1).indices == labels).sum() / batch_size\n",
    "        \n",
    "\n",
    "        \n",
    "    m_loss /= len(train_loader)\n",
    "    acc /= len(train_loader)\n",
    "    \n",
    "    print(f\"epoch {i} - loss: {m_loss}\")\n",
    "    print(f\"train accuracy {acc}\")\n",
    "    \n",
    "    if (i+1)%1 == 0:\n",
    "        acc = 0\n",
    "        m_loss=0\n",
    "        minkpointnet.eval()\n",
    "        with torch.no_grad():\n",
    "            for ebatch in eval_loader:\n",
    "            \n",
    "                labels = ebatch[\"labels\"].long().to(device)\n",
    "\n",
    "                batch_size = labels.shape[0]\n",
    "\n",
    "                minknet_input = create_input_batch(\n",
    "                    ebatch, \n",
    "                    device=device,\n",
    "                    quantization_size=0.05\n",
    "                )\n",
    "\n",
    "\n",
    "                pred = minkpointnet(minknet_input)\n",
    "\n",
    "                loss = cls_loss(pred, labels)\n",
    "\n",
    "                m_loss += loss.item() #/ batch_size\n",
    "\n",
    "                acc += (torch.max(pred, dim=-1).indices == labels).sum() / batch_size\n",
    "\n",
    "\n",
    "            m_loss /= len(eval_loader)\n",
    "            acc /= len(eval_loader)\n",
    "            print(f\"validation_loss: {m_loss}\")\n",
    "            print(f\"validation_acc : {acc}\")\n",
    "        minkpointnet.train()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3299e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a confusion matrix for the data\n",
    "# Creating a dataloader with batch size = 1\n",
    "conf_loader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn = minkowski_collate, num_workers=8)\n",
    "\n",
    "preds = torch.zeros(40, 40)\n",
    "\n",
    "acc = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    minkpointnet.eval()\n",
    "    \n",
    "    for batch in tqdm(conf_loader):\n",
    "    \n",
    "        labels = batch[\"labels\"].long().to(device)\n",
    "        \n",
    "        batch_size = labels.shape[0]\n",
    "        \n",
    "        minknet_input = create_input_batch(\n",
    "            batch, \n",
    "            device=device,\n",
    "            quantization_size=0.05\n",
    "        )\n",
    "        \n",
    "        pred = minkpointnet(minknet_input)\n",
    "        \n",
    "        pred_ind = pred.max(dim=-1).indices.cpu()\n",
    "\n",
    "        preds[labels.item(), pred_ind] += 1\n",
    "        \n",
    "        loss = cls_loss(pred, labels)\n",
    "                \n",
    "        m_loss += loss.item() #/ batch_size\n",
    "        \n",
    "        acc += (torch.max(pred, dim=-1).indices == labels).sum() / batch_size\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b605c8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def confusion_matrix(data, xlabels: list, ylabels: list):\n",
    "\n",
    "    assert data.dim() in [2,3], \"The input must be a square matrix or a batch of square matrices\"\n",
    "    \n",
    "    if data.dim() == 3:\n",
    "\n",
    "        B, N, M = data.shape\n",
    "        assert N == M\n",
    "        assert len(xlabels) == len(ylabels) == B or len(xlabels) == len(ylabels) == N\n",
    "        try:\n",
    "            for subx, suby in zip(xlabels, ylabels):\n",
    "                assert len(subx) ==  N and len(suby) == N\n",
    "        except AssertionError:\n",
    "            xlabels = [xlabels for i in range(B)]\n",
    "            ylabels = [ylabels for i in range(B)]\n",
    "\n",
    "\n",
    "        rows, cols = 1, B\n",
    "        if B % 3 == 0:\n",
    "            rows, cols = 3, B//3\n",
    "        elif B % 2 == 0:\n",
    "            rows, cols = 2, B//2\n",
    "\n",
    "        ticks = [i for i in range(N)]\n",
    "\n",
    "        fig, axes = plt.subplots(rows, cols)\n",
    "        for r in range(rows):\n",
    "            for c in range(cols):\n",
    "                i = r*cols + c\n",
    "                axes[r, c].matshow(data[i])\n",
    "                plt.xticks(ticks = ticks, labels = xlabels[i])\n",
    "                plt.yticks(ticks = ticks, labels = ylabels[i])\n",
    "        \n",
    "    else:\n",
    "\n",
    "        N, M = data.shape\n",
    "        assert N == M\n",
    "        assert len(xlabels) == N and len(ylabels) == N\n",
    "\n",
    "        ticks = [i for i in range(N)]\n",
    "\n",
    "        plt.matshow(data)\n",
    "        plt.xticks(ticks = ticks, labels = xlabels)\n",
    "        plt.yticks(ticks = ticks, labels = ylabels)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5601496b",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc / len(conf_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f388c381",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('vvrenv': conda)",
   "language": "python",
   "name": "python3812jvsc74a57bd0b6c2dec4bbf77c09193d7602271711a5cc1ce866b52e710bc0ceb900b29ba2ee"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
